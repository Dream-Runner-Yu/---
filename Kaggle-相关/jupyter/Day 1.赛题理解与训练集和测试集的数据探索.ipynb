{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f47601-783e-420c-a362-a06eff81f4e2",
   "metadata": {},
   "source": [
    "# <font face=\"仿宋\">课程说明："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c3a33b-7d03-49e8-ba30-d99d4ab80f99",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">小伙伴好呀\\~欢迎来到《2021机器学习实战训练营》（第二期）试学体验课！我是课程主讲老师，九天。       \n",
    "&emsp;&emsp;本次体验课为期三天（10月21-23号），期间每晚8点在我的B站直播间公开直播，直播间地址:https://live.bilibili.com/22678166      \n",
    "&emsp;&emsp;本期公开课将围绕一项kaggle竞赛案例进行深度剖析，并据此讨论算法竞赛中机器学习的一般建模流程，以及当前机器学习进行预测时最为有效的技术手段，也就是特征工程方法和集成算法的相关应用，对算法和竞赛感兴趣的小伙伴，欢迎积极参与讨论哦\\~        \n",
    "&emsp;&emsp;课程资料领取/数据技术交流/付费课程信息，扫码添加客服“小可爱”获取哦~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ab37e-8c6f-4163-a12c-1cd971bae076",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.loli.net/2021/10/20/ZWTgxSiNY1db9eL.png\" alt=\"二维码\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d95d033-e3b7-4a4b-ae56-47ad5ad8eb59",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font face=\"仿宋\">另外，《机器学习实战训练营》（第二期）本月25号即将开课，十八周80+课时体系大课限时半价，扫码咨询小可爱回复“优惠”，还可领取额外折上折优惠，课程主页：https://appze9inzwc2314.pc.xiaoe-tech.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e08eec8-418a-4ee8-84e2-aab0550b1004",
   "metadata": {},
   "source": [
    "# <center>【Kaggle】Elo Merchant Category Recommendation    \n",
    "# <center> 竞赛案例解析公开课"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd783f3a-dfca-49cb-80f9-81c5dcea322a",
   "metadata": {},
   "source": [
    "# <center>Day 1.赛题分析与训练集和测试集数据探索"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e147fc1-e03a-4bfa-b5cb-57b094e10e71",
   "metadata": {},
   "source": [
    "## 一、Kaggle基本使用方法、赛题背景以及数据获取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c17da-588b-4903-ac9f-ef388ee97810",
   "metadata": {},
   "source": [
    "&emsp;&emsp;本次公开课的案例选自Kaggle竞赛赛题：[Elo Merchant Category Recommendation](https://www.kaggle.com/c/elo-merchant-category-recommendation)。该比赛举办于2019年，本次比赛总共持续将近4个月时间，有共计四千余只战队参赛，奖金池高达5万美元，属于算法竞赛中的大型赛事。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c8b58-f8ae-41cc-8c7c-2a0cb817f915",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.loli.net/2021/10/20/DplYIfdsh3JCbEO.png\" alt=\"1\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189fa579-8983-4a5b-badd-f4085ebe1c03",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在正式解读该赛题背景之前，我们需要简单介绍关于如何从Kaggle平台上获取关于该赛题的相关资源与赛题说明，也就是关于Kaggle的一些基本使用方法。值得注意的是，尽管我们能够从很多其他地方获取关于该赛题的说明和数据，但Kaggle官方给出的说明始终是最权威并且需要深入分析的。并且Kaggle作为官方竞赛平台，本身也提供了非常多的交流、探讨与协作的机会，通过参与讨论以及参考他人提交的代码，也将有助于深入理解业务背景、拓展建模思路。可以说Kaggle平台的基本使用方法，是每位想要深入学习竞赛案例的同学必须掌握的内容。学会合理使用Kaggle，也将非常有助于日后的学习。        \n",
    "&emsp;&emsp;当然，Kaggle平台功能众多，此处我们先介绍关于获取赛题说明、赛题数据以及参与讨论的相关方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1a5d0d-1126-4e65-9f89-24b122d42d7b",
   "metadata": {},
   "source": [
    "- Kaggle基本功能介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b55a54-c39a-48cf-a6a5-454a6762c385",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先，我们可以在此处看到关于Elo Merchant Category Recommendation竞赛的基本说明：https://www.kaggle.com/c/elo-merchant-category-recommendation 。并且在该页面，我们能够获取到与之相关的所有核心信息，当然，由于是完结赛事，相比组队/参与竞赛/提交代码等功能，我们更关注如何获取数据、赛题说明以及参与讨论、查看代码分享等功能，相关功能入口如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fff50b-6453-4e12-8d77-9c1ca4f35c16",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.loli.net/2021/10/20/tAOL9DrYoKReGUu.png\" alt=\"2\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826bdd8a-dea7-49be-8609-9d99b9e68026",
   "metadata": {},
   "source": [
    "### 1.赛题背景简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4c5af3-2c72-41df-869e-9d0e4fe6a2c8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在了解Kaggle的基本功能布局后，接下来我们来简单查看赛题的基本介绍。当然，此处的简单也并非主动有意为之，而是在官方给出的说明中，其实并没有太多有效的业务信息。我们可以先从上述Overview的内容入手进行了解："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da51a7b-f920-4147-8f37-00be70b27fdb",
   "metadata": {},
   "source": [
    "- 巴西支付品牌Elo        \n",
    "&emsp;&emsp;本次竞赛其实是由巴西最大的支付品牌之一的Elo和Kaggle合作举办的比赛，奖金和数据都由Elo公司提供。谈到支付品牌，国内用户首先会想到类似支付宝、PayPal这些带有浓烈互联网色彩的支付品牌，但是在巴西，线上支付更多是由本地银行主导，且线上支付的信贷产品也主要以信用卡为主。Elo就是这样的一家公司，在2011年由巴西三家主要银行合资创立，主要负责线上支付业务，并且以信用卡作为核心金融产品，目前已发放超过1.1亿张信用卡，是巴西最大的本地在线支付品牌之一。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d82f763-22d7-45a0-b95b-a8c5503c3189",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.loli.net/2021/10/20/SwXTECfVOHLMAr7.png\" alt=\"4\" style=\"zoom:25%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39967a29-255a-482b-96bf-0a3b47509991",
   "metadata": {},
   "source": [
    "并且，Elo不仅是支付入口，更是一个“o2o”平台，通过App，用户可以查阅本地餐饮旅馆电影旅游机票等各项服务，并支持信用卡在线支付。形象点理解，就好比把美团主页移到支付宝，并且支付宝没有花呗，取而代之的是自己发行的信用卡。或者更加形象的理解，就类似国内招行信用卡掌上生活的业务模式："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b02226d-53fb-4b52-8731-d2c0cec102b9",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.loli.net/2021/10/20/sAQmTcvw5P8fHdV.png\" alt=\"3\" style=\"zoom:20%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8ec765-1128-4109-8312-c1c01e672472",
   "metadata": {},
   "source": [
    "- 业务目标：更好的进行本地服务推荐      \n",
    "&emsp;&emsp;在官方给出的说明中，我们不难发现，Elo使用机器学习算法技术的核心目的，是为了更好的在App内为用户推荐当地吃穿住行的商家服务，包括热门餐厅展示、优惠折扣提醒等（非常类似上图掌上生活首页的推荐）。也就是说，其根本目的是为了推荐，或者说为每个用户进行更加个性化的推荐，也就是赛题标题中的所写的：Merchant Category Recommendation（商户类别推荐）。但是，需要注意的是，本次竞赛的建模目标却和推荐系统并不直接相关。赛题说明中，在介绍完业务目标之后，紧接着就介绍了本次赛题的目标：对用户的忠诚度评分进行预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59c39b3-1239-4b1e-923d-6a0b4b7d8dcf",
   "metadata": {},
   "source": [
    "- 算法目标：用户忠诚度评分预测        \n",
    "&emsp;&emsp;所谓用户忠诚度评分，通过后续查看Evaluation不难发现，其实就是对每个用户的评分进行预测，本质上是个回归问题。      \n",
    "&emsp;&emsp;相信刚接触到本次赛题的小伙伴，一定会觉得赛题说明和建模目标有些“文不对题”，毕竟用户忠诚度评分貌似和个性化推荐并无直接关系，尤其此处用户忠诚度评分并不是针对某类商品或者某类商家的忠诚度评分。        \n",
    "&emsp;&emsp;围绕这个问题，赛题说明给出了非常笼统的解释，只是简单说到通过对用户忠诚度评分，能够为其提供最相关的机会（serve the most relevant opportunities to individuals），或者可以理解成是用户较为中意的线下服务，并且帮助ELo节省活动成本，为用户提供更好的体验。其实也就等于是没有解释忠诚度评分和推荐系统到底是什么关系。Kaggle本赛题论坛上也有很多相关讨论帖，官方给出的解释大意是通过忠诚度评分给用户推荐商铺和商品，这个过程并不是一个传统的协同过滤或者推荐系统进行推荐的过程，无论如何，先做好忠诚度预测就好。        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a051046-9c9e-4da1-8c66-f487647b7d45",
   "metadata": {},
   "source": [
    "- 借助论坛挖掘更多信息      \n",
    "&emsp;&emsp;当然，如果你有类似疑问，那么一定也是很多竞赛小伙伴的疑问，并且很多问题肯定在论坛上有非常热烈的讨论。因此无论是在赛题理解，还是实际建模过程中，遇到问题第一时间还是应该回到Kaggle论坛上来查阅资料或者参与讨论。关于忠诚度评分如何能够解决精准推荐的问题问题，本次竞赛的榜一大佬给出了自己的理解：https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/75034#442077 。此外，在论坛置顶中贴中的部分评论也值得一看，https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/72993 。感兴趣小伙伴可以自行解读，总结来说有以下几点核心信息需要补充：      \n",
    "&emsp;&emsp;(1).对于Elo来说，其实是有一套计算用户忠诚度的公式的，这也就是标签的由来。因此我们在进行建模的时候实际上是为了建立一套计算过程，尽可能逼近这个公式的计算结果；      \n",
    "&emsp;&emsp;(2).但是，通过简单尝试不难发现，Elo内部计算忠诚度的公式极有可能完全不是我们现在看到的这些字段，也就是说我们需要使用另一套字段去尽可能还原Elo内部计算忠诚度的过程；      \n",
    "&emsp;&emsp;(3).忠诚度评分如何影响精准推荐，这个过程极有可能类似游戏匹配机制，即不同分数段、不同战绩有可能匹配到不同的对手，而本次竞赛中其实只有关于用户一方分数的计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaf5681-59c8-4445-810e-74fcbccfdc28",
   "metadata": {},
   "source": [
    "&emsp;&emsp;至此，我们就完成了赛题背景所有关键信息的分析与挖掘。一定程度挖掘赛题业务背景将有助于辅助后续建模工作的展开，根据目前所掌握的信息，可能需要用到用户画像中的用户价值分层方法来辅助进行特征工程，类似RFM模型中的关键指标。当然，更多信息大家也可以参考论坛中其他帖子。不过值得一提的是，算法竞赛中对于赛题背景的业务挖掘点到即止即可，一方面是因为出于保密性要求，竞赛主办方会刻意隐藏部分信息，脱敏、创造新字段、甚至是不给出字段解释都是常见方法；另一方面，对于机器学习来说，重数字规律可能会强于重业务逻辑。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab2074e-7ae0-4bc4-b631-8899783b660b",
   "metadata": {},
   "source": [
    "### 2.数据获取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c123e9-4635-4178-8e9b-711c84bb2e4c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在理解赛题背景之后，接下来我们需要获取本次竞赛的数据。我们可以从Data页面中获取竞赛数据："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c742100-6b15-4de8-bfa7-70284b31077d",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.loli.net/2021/10/20/ciuCB3gvlJ7tLhj.png\" alt=\"5\" style=\"zoom:45%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6146b536-0e5c-4481-8d72-007a691cafbe",
   "metadata": {},
   "source": [
    "&emsp;&emsp;我们可以通过页面选项简单观察各数据集，在实际数据下载过程中，可以通过两种方法进行下载，一种是通过页面按钮（Download ALL）进行下载，另一种则是通过命令行进行下载。相比网页下载，命令行下载会更加通用、高效和稳定，此处简单介绍如何通过命令行方法获取数据："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d386c8a-a2ba-419e-95d1-e169542682d1",
   "metadata": {},
   "source": [
    "- 安装kaggle包"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8568eed-1eb6-4430-9a38-7c11337b2e7f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;要使用命令行下载数据，首先需要安装kaggle包。和其他所有第三方库的安装方法一样，使用pip安装即可。即打开命令行，输入下述命令："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8674f2f7-ec2a-4460-8f00-075b682ec78c",
   "metadata": {},
   "source": [
    "```shell\n",
    "pip install kaggle\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1b70ce-218e-40e2-820e-f3564e4d4f59",
   "metadata": {},
   "source": [
    "- 获取kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ec0cb4-27c9-4c54-94ee-8f96eb907143",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在安装完成kaggle之后，进入Kaggle的个人主页（点击右上角头像），点击Create New API Token，则可创建一个kaggle.json文件，并自动开始下载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03ebba4-0ed9-4009-b869-5cd16576451f",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.loli.net/2021/10/20/zBmnXEA2RkrDaxv.png\" alt=\"image-20211020201319108\" style=\"zoom:30%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424f2ab-188d-421c-9839-06b2f9a64a57",
   "metadata": {},
   "source": [
    "- 将kaggle.json文件移动到.kaggle文件夹内"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0044337d-749b-4c91-a1a8-254264648238",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，将下载好的kaggle.json文件移动到当前用户主目录下的.kaggle文件夹内，该文件夹在安装完kaggle后会自动创建："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63666c88-7229-454e-a5fd-1ba5e358c1eb",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.loli.net/2021/10/20/1IOBbri3fzVWmCh.png\" alt=\"image-20211020201725672\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f89657d-b731-4875-9b85-c801ad25d038",
   "metadata": {},
   "source": [
    "- 修改默认下载路径"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14bd06a-c915-4537-8bb8-2695106e57d9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，很多时候我们需要修改数据集默认下载路径，此处可以通过下述命令进行修改："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95676ad1-66bb-4b06-8801-a7d39abcb1ee",
   "metadata": {},
   "source": [
    "```shell\n",
    "kaggle config set -n path -v<路径>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91e2400-9f95-4287-8913-2eb45213c165",
   "metadata": {},
   "source": [
    "- 使用命令行进行下载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9937cbc2-4dc7-482c-bb6b-ca5d084871bd",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，复制数据集页面的命令行，即可开始进行下载。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3117aa0-2a87-461f-b485-01868d026569",
   "metadata": {},
   "source": [
    "```shell\n",
    "kaggle competitions download -c elo-merchant-category-recommendation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a359853-51f9-4acd-9ee3-3f9cfc282694",
   "metadata": {},
   "source": [
    "下载后解压，全部7个数据文件如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa2f4b7-912c-4ce4-adaa-b0f681cb6786",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.loli.net/2021/10/21/lnVaXHg24vwM1fQ.png\" alt=\"image-20211021110745226\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1535edcc-980e-4966-a202-2a03fb4be3b9",
   "metadata": {},
   "source": [
    "> 通过各种方法均无法下载的同学，可以扫描开始二维码，联系客服小可爱，回复“kaggle”获取本案例数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f8afe7-5d2b-4cc0-96e6-86496f49e6cb",
   "metadata": {},
   "source": [
    "### 3.数据表简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722bfd72-0c2a-4a21-b935-de16403e20a2",
   "metadata": {},
   "source": [
    "&emsp;&emsp;能够看到，本次赛题数据较多、数据量也相对较大，部分数据甚至无法直接通过普通Excel直接打开。接下来我们快速了解每个数据集的基本含义："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50e8acc-e97a-41d7-8294-b77c213a3393",
   "metadata": {},
   "source": [
    "&emsp;&emsp;总的来说，上述7个数据文件大概可以分为三类，其一是基本信息类数据集，包括Data_Dictionary和sample_submission。其中Data_Dictionary数据集是所有数据的数据字典，即包括了所有数据各字段的含义，而sample_submission则是提交结果时的范例数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a06f86-1b1e-4d47-b587-e7ae8a1fb92b",
   "metadata": {},
   "source": [
    "- Data Dictionary/Data_Dictionary：数据字典"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a77c211-1608-4120-a1bb-3c93865b9052",
   "metadata": {},
   "source": [
    "&emsp;&emsp;所有其他数据表中每个字段的含义，相当于是其他各数据表的说明书。数据字典包含多个sheet，每个sheet对应一个数据表的字段和解释："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4697e810-77da-4114-9c4d-1d7b5f1061e7",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.loli.net/2021/10/20/ilGc7hrAwMZNpS8.png\" alt=\"image-20211020204816830\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4786cc9d-a5c5-430a-aff5-41bbea9d9a8d",
   "metadata": {},
   "source": [
    "> 其他数据集字段有重复，因此未在数据字典中列出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94ff694-9b03-4595-9b65-eeffcf54a92f",
   "metadata": {},
   "source": [
    "当然，我们可以通过如下方式直接使用pandas进行读取："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd86363-b6af-46ed-aca3-803f7c7e4d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42631f52-842c-4aea-a2ff-11f01cd5c69c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './eloData/Data_Dictionary.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-b56fbf1629af>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# 第三行开始读取，读取train表\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_excel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'./eloData/Data_Dictionary.xlsx'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mheader\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msheet_name\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'train'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\bw_pinglun\\lib\\site-packages\\pandas\\util\\_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    310\u001B[0m                 )\n\u001B[1;32m--> 311\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    312\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    313\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\bw_pinglun\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001B[0m in \u001B[0;36mread_excel\u001B[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001B[0m\n\u001B[0;32m    362\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mExcelFile\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    363\u001B[0m         \u001B[0mshould_close\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 364\u001B[1;33m         \u001B[0mio\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mExcelFile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstorage_options\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstorage_options\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mengine\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    365\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mengine\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mengine\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[0mio\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    366\u001B[0m         raise ValueError(\n",
      "\u001B[1;32m~\\.conda\\envs\\bw_pinglun\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, path_or_buffer, engine, storage_options)\u001B[0m\n\u001B[0;32m   1190\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1191\u001B[0m                 ext = inspect_excel_format(\n\u001B[1;32m-> 1192\u001B[1;33m                     \u001B[0mcontent_or_path\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mpath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstorage_options\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstorage_options\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1193\u001B[0m                 )\n\u001B[0;32m   1194\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mext\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\bw_pinglun\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001B[0m in \u001B[0;36minspect_excel_format\u001B[1;34m(content_or_path, storage_options)\u001B[0m\n\u001B[0;32m   1069\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1070\u001B[0m     with get_handle(\n\u001B[1;32m-> 1071\u001B[1;33m         \u001B[0mcontent_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"rb\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstorage_options\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstorage_options\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mis_text\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1072\u001B[0m     ) as handle:\n\u001B[0;32m   1073\u001B[0m         \u001B[0mstream\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mhandle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhandle\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\bw_pinglun\\lib\\site-packages\\pandas\\io\\common.py\u001B[0m in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    709\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    710\u001B[0m             \u001B[1;31m# Binary mode\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 711\u001B[1;33m             \u001B[0mhandle\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhandle\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    712\u001B[0m         \u001B[0mhandles\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhandle\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    713\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './eloData/Data_Dictionary.xlsx'"
     ]
    }
   ],
   "source": [
    "# 第三行开始读取，读取train表\n",
    "pd.read_excel('./eloData/Data_Dictionary.xlsx', header=2, sheet_name='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1037ec04-17c9-4268-97cc-5bb23e6780ec",
   "metadata": {},
   "source": [
    "- sample_submission：正确提交结果范例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a0c5ba-b7ea-4dc9-bffd-b8f79e7ef0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据文件\n",
    "pd.read_csv('./eloData/sample_submission.csv', header=0).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32c140c-b51a-4fa4-98d0-b962e19ca7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看数据集基本信息\n",
    "pd.read_csv('./eloData/sample_submission.csv', header=0).info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0df70a-bb99-4f84-a2cd-433e353f2fca",
   "metadata": {},
   "source": [
    "&emsp;&emsp;最终建模结果提交格式，也就是以“一个id”+“对应预测结果”的格式进行提交。据此我们也能发现，实际上我们是需要预测每个card_id的用户忠诚度评分。我们也可以在竞赛Kaggle主页上查看提交结果格式和评估指标："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f252fbd8-46fd-4496-91ea-cdc1ca75bc3d",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i.loli.net/2021/10/20/tveFu12sbNcAfwH.png\" alt=\"image-20211020211206067\" style=\"zoom:35%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29dda56-2a28-4159-ae01-3286e077e9cf",
   "metadata": {},
   "source": [
    "RMSE的计算过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f8d698-10de-4dbc-bed2-6c1962e87909",
   "metadata": {},
   "source": [
    "$$RMSE= \\sqrt{\\frac{1}{n}\\sum^n_{i=1}(y_i-\\hat y_i)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3623f827-3302-4ccb-90ed-99e98203d02a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后就是完成比赛的必要数据，也就是train和test两个数据集。顾名思义，train数据集就是训练数据，test就是测试数据集，二者特征一致，极简情况下我们可以直接在train上训练模型，在test上进行预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0748b9-c7a7-42e0-a75b-679e8c560901",
   "metadata": {},
   "source": [
    "&emsp;&emsp;最后一类则是补充数据集，也就是 'historical_transactions.csv'、'new_merchant_transactions.csv'和'merchants.csv',其中前两个数据集记录了训练集和测试集信用卡的消费记录，而最后一个数据集则是前两个数据集中商铺信息（某特征）的进一步解释。在实际建模过程中，纳入更多数据进行规律挖掘，则有可能达到更好的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b011ae0a-8fd2-4de5-901c-e178dbf6ddd9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在接下来的数据探索中，我们将首先重点解读train和test数据集的相关信息，然后再进一步探索其他三个补充数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e5d4f6-2bb9-4224-b819-f27a12c05845",
   "metadata": {},
   "source": [
    "## 二、train和test解读与初步探索"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5905bcd5-eac4-4c0f-a079-ad5824ba9a5e",
   "metadata": {},
   "source": [
    "### 1.train和test数据集解读"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f615ed7-5bf0-4e97-9f68-dabd72c8f742",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先先进行数据读取。当然考虑到后续数据集规模较大，我们可以提前导入gc包以进行内存管理。在实际清理内存时，我们可以先使用del删除对象、再使用gc.collect()，以达到手动清理内存的目的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe664236-db30-4aa5-bf0c-2ef09d45a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ef41b-8c18-49f3-bb38-f63844d7acf8",
   "metadata": {},
   "source": [
    "首先进行数据读取："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641588cf-79a5-499e-844f-b9a79aace914",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./eloData/train.csv')\n",
    "test =  pd.read_csv('./eloData/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5768fc0e-3d9a-47f2-95ac-ff4cf2e4f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看数据集规模\n",
    "(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e4ffab-ddd6-4027-ad44-16bd5b46e907",
   "metadata": {},
   "source": [
    "然后依次对其进行解读。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbaa560-5484-4c93-b192-2592e9b652f3",
   "metadata": {},
   "source": [
    "- train：训练数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6408f8-6fe1-43c4-9af1-32cc971d46fa",
   "metadata": {},
   "source": [
    "&emsp;&emsp;训练数据基本情况如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d339bd51-c78a-49f5-ad54-a94d88c2c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看前5条数据\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa770bc5-6889-422c-ba80-a3bf79feca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看数据集信息\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfff31c-5ab8-4782-a75b-d24ea44e0298",
   "metadata": {},
   "source": [
    "回顾数据字典中的train表，查看train数据集中各字段解释："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b764775-c2d9-462b-b74a-5a198abc4a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel('./eloData/Data_Dictionary.xlsx', header=2, sheet_name='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd4fe8b-27f7-40ab-b1d6-274f2c748b11",
   "metadata": {},
   "source": [
    "实际含义如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa3edb-6096-4e50-a9e1-ff745e4478c9",
   "metadata": {},
   "source": [
    "| 字段 | 解释 |\n",
    "| ------ | ------ |\n",
    "| card_id | 第一无二的信用卡标志 |\n",
    "| first_active_month | 信用卡首次激活时间，按照类似2017-02排列 |\n",
    "| feature_1/2/3 | 匿名特征（不带明显业务背景或人工合成的特征） |\n",
    "| target | 标签，忠诚度评分 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccaa588-2d65-4de7-9f87-15562d9fce67",
   "metadata": {},
   "source": [
    "- test：测试数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537f97a3-a67e-4e52-97a8-07353fb0c951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看前5条数据，和submission一致\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1b1bb7-6ae4-40f5-bc5c-fbb989fefcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad41223-4629-4e48-9941-49bbaebe5a91",
   "metadata": {},
   "source": [
    "并且各字段的解释和train一致。实际比赛过程中，由于测试集的标签“不可知”，所以需要在训练集train上划分出验证集来进行模型泛化能力评估。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee0f9f4-9f24-40e7-9485-1e0599caad7f",
   "metadata": {},
   "source": [
    "### 2.数据质量分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0d656a-3866-4036-8947-5fd3a0da0d0b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来简单数据探索。在实际建模过程中，首先我们会先校验数据的正确性，并检验缺失值、异常值等情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3341fd1d-abb7-46a2-aab2-bc49ded4132e",
   "metadata": {},
   "source": [
    "- 数据正确性校验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9dde8e-e89b-467c-8bf9-e73f71116364",
   "metadata": {},
   "source": [
    "&emsp;&emsp;所谓数据正确性，指的是数据本身是否符合基本逻辑，例如此处信用卡id作为建模分析对象独一无二的标识，我们需要验证其是否确实独一无二，并且训练集和测试集信用卡id无重复。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaa0897-43ae-4604-9eec-53a517397553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检验训练集id无重复\n",
    "train['card_id'].nunique() == train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcbefec-2a31-488a-9d86-530c23b794d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检验测试集id无重复\n",
    "test['card_id'].nunique() == test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c658e6-8aff-44bd-9b6f-0fabb3d87cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检验训练集和测试集id都是唯一值\n",
    "test['card_id'].nunique()+ train['card_id'].nunique()  == len(set(test['card_id'].values.tolist() + train['card_id'].values.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5777ba7-05db-4943-bebb-976ba8ec6a6d",
   "metadata": {},
   "source": [
    "- 检验数据缺失情况"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef76bb4-ce96-44c2-a94d-bf9d4c324d5e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，进一步检验数据确实情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c73b3-f672-45a3-b2a0-ae2646e611bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按列求缺失值并汇总\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434796a4-fc17-41b0-89f4-0e690b7b2130",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463552f6-c834-4de1-91a0-7b72b11859dc",
   "metadata": {},
   "source": [
    "能够发现数据集基本无缺失值，测试集中的唯一一个缺失值我们可以通过多种方式来进行填补，整体来说一条缺失值并不会对整体建模造成太大影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5709b6f-6be2-4ef2-9fc9-e8f3a373534a",
   "metadata": {},
   "source": [
    "- 异常值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217a96b7-9886-4d0d-a3ef-33908614a92a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来进行异常值检验。由于我们尚未对数据集特征进行预处理，因此我们先查看标签列的异常值情况。首先我们可以用describe()方法查看这一列的基本统计信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c668ba9-b210-4955-bfa3-55391c5adeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = train['target'].describe()\n",
    "statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390fee7f-7941-4c6b-8a3f-55f3ea61ad08",
   "metadata": {},
   "source": [
    "由于该列是连续变量，我们可以借助概率密度直方图进行分布的观察："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef4fa3-da2d-443f-8c97-a4a994ed1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40822179-b1ef-4d29-a419-d650744e2469",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.histplot(train['target'], kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139664ec-2662-4472-9a2b-c4aea081e36f",
   "metadata": {},
   "source": [
    "能够发现，大部分用户忠诚度评分都集中在[-10,10]之间，并且基本符合正态分布，唯一需要注意的是有个别异常值取值在-30以下，该数据在后续分析中需要额外注意。我们可以简单查看有多少用户的标签数值是小于30的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ed24b5-f5a8-44af-a664-63b9c5e693ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train['target'] < -30).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff435d0-041f-4c7d-a2df-e7725e7d98f8",
   "metadata": {},
   "source": [
    "约占整体20万数据的1%。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2edd6b8-b592-46fd-b869-6b3aff341101",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，对于连续变量，一般可以采用$3\\delta$原则进行异常值识别，此处我们也可以简单计算下异常值范围："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f691c7-9c18-4589-9a29-a6f6fd29afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics.loc['mean'] - 3 * statistics.loc['std']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40adbbd9-8d43-4584-9edc-ad8a6d2fe56c",
   "metadata": {},
   "source": [
    "即取值大于或小于11.94即可视作异常。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f8fcd5-409f-4b7f-b35a-66559b7cf020",
   "metadata": {},
   "source": [
    "- 异常值分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d005d560-c6ff-456f-a195-6daa22d8646f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;需要注意的是，此处我们是围绕标签进行的异常值检测，而本案例中标签并不是自然数值测量或统计的结果（如消费金额、身高体重等），而是通过某种公式人工计算得出（详见赛题分析）。出现如此离群点极有可能是某类特殊用户的标记。因此不宜进行异常值处理，而应该将其单独视作特殊的一类，在后续建模分析时候单独对此类用户进行特征提取与建模分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31779d21-ca6c-401a-ac12-b4bfc53f943c",
   "metadata": {},
   "source": [
    "### 4.规律一致性分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b5b853-809d-4059-ba47-cc2fd432750a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，进行训练集和测试集的规律一致性分析。        \n",
    "&emsp;&emsp;所谓规律一致性，指的是需要对训练集和测试集特征数据的分布进行简单比对，以“确定”两组数据是否诞生于同一个总体，即两组数据是否都遵循着背后总体的规律，即两组数据是否存在着规律一致性。      \n",
    "&emsp;&emsp;我们知道，尽管机器学习并不强调样本-总体的概念，但在训练集上挖掘到的规律要在测试集上起到预测效果，就必须要求这两部分数据受到相同规律的影响。一般来说，对于标签未知的测试集，我们可以通过特征的分布规律来判断两组数据是否取自同一总体。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b4773b-9959-4964-afea-b38157c8b933",
   "metadata": {},
   "source": [
    "- 单变量分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23124eeb-a509-4441-87c3-7d57824384f8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先我们先进行简单的单变量分布规律的对比。由于数据集中四个变量都是离散型变量，因此其分布规律我们可以通过相对占比分布（某种意义上来说也就是概率分布）来进行比较。        \n",
    "&emsp;&emsp;例如首先我们查看首次激活月份的相对占比分布可以通过如下代码实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f9996e-3787-4439-bb49-1db64a70da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征列名\n",
    "features = ['first_active_month','feature_1','feature_2','feature_3']\n",
    "\n",
    "# 训练集/测试集样本总数\n",
    "train_count = train.shape[0]\n",
    "test_count = test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b6d45-b417-4202-8a56-91aec0281e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同取值水平汇总后排序再除以样本总数\n",
    "train['first_active_month'].value_counts().sort_index()/train_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030a144b-27a3-47b3-95f3-d16731cf3dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分布图如下所示：\n",
    "(train['first_active_month'].value_counts().sort_index()/train_count).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1596d89-d52c-4be9-b5a0-bf19bba13b0c",
   "metadata": {},
   "source": [
    "当然，我们需要同时对比训练集和测试集的四个特征，可以通过如下代码实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf364cb-e527-4a78-b1eb-91995e2b8f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    (train[feature].value_counts().sort_index()/train_count).plot()\n",
    "    (test[feature].value_counts().sort_index()/test_count).plot()\n",
    "    plt.legend(['train','test'])\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('ratio')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f045e2-9ed0-4d9e-9c30-cf56335c4b41",
   "metadata": {},
   "source": [
    "能够发现，两组数据的单变量分布基本一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980790d9-73e3-4961-b812-7d363ea6c245",
   "metadata": {},
   "source": [
    "- 多变量联合分布"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bd14ee-af3f-4a01-9ed4-1f85f33ef016",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，我们进一步查看联合变量分布。所谓联合概率分布，指的是将离散变量两两组合，然后查看这个新变量的相对占比分布。例如特征1有0/1两个取值水平，特征2有A/B两个取值水平，则联合分布中就将存在0A、0B、1A、1B四种不同取值水平，然后进一步查看这四种不同取值水平出现的分布情况。        \n",
    "&emsp;&emsp;首先我们可以创建如下函数以实现两个变量“联合”的目的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa6da6-c93a-4401-beb4-c493b18cf049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_feature(df):\n",
    "    cols = df.columns\n",
    "    feature1 = df[cols[0]].astype(str).values.tolist()\n",
    "    feature2 = df[cols[1]].astype(str).values.tolist()\n",
    "    return pd.Series([feature1[i]+'&'+feature2[i] for i in range(df.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe1aba-2fdb-4b91-a451-aaa061529893",
   "metadata": {},
   "source": [
    "简单测试函数效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901d4477-d28b-49dc-99af-f4eaf7e9cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选取两个特征\n",
    "cols = [features[0], features[1]]\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c5f24-6780-48db-b091-ae26306e94d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看合并后结果\n",
    "train_com = combine_feature(train[cols])\n",
    "train_com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707c5145-74ab-4cfb-9d80-5299531d9cce",
   "metadata": {},
   "source": [
    "进一步计算占比分布："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d47511e-0e03-4d44-be91-ef4f1086613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dis = train_com.value_counts().sort_index()/train_count\n",
    "train_dis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf8a826-f30d-484a-9338-17cf10fb1284",
   "metadata": {},
   "source": [
    "当然，也可以对测试集进行相同操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572bcba4-3563-4d48-a33d-f874342e2ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dis = combine_feature(test[cols]).value_counts().sort_index()/test_count\n",
    "train_dis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602e5c96-852f-49f5-8b70-367afeca024c",
   "metadata": {},
   "source": [
    "然后对比二者分布："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fdb836-2118-4b78-af04-b708342905b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建新的index\n",
    "index_dis = pd.Series(train_dis.index.tolist() + test_dis.index.tolist()).drop_duplicates().sort_values()\n",
    "\n",
    "# 对缺失值填补为0\n",
    "(index_dis.map(train_dis).fillna(0)).plot()\n",
    "(index_dis.map(train_dis).fillna(0)).plot()\n",
    "\n",
    "# 绘图\n",
    "plt.legend(['train','test'])\n",
    "plt.xlabel('&'.join(cols))\n",
    "plt.ylabel('ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede5faff-8f3a-49e7-9979-3fde8fb50f4f",
   "metadata": {},
   "source": [
    "能够发现其分布基本一致。当然我们可以通过如下代码快速致性所有两两变量联合分布的比较："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00359da7-d18d-4a8d-aa76-98c61891008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(features)\n",
    "for i in range(n-1):\n",
    "    for j in range(i+1, n):\n",
    "        cols = [features[i], features[j]]\n",
    "        print(cols)\n",
    "        train_dis = combine_feature(train[cols]).value_counts().sort_index()/train_count\n",
    "        test_dis = combine_feature(test[cols]).value_counts().sort_index()/test_count\n",
    "        index_dis = pd.Series(train_dis.index.tolist() + test_dis.index.tolist()).drop_duplicates().sort_values()\n",
    "        (index_dis.map(train_dis).fillna(0)).plot()\n",
    "        (index_dis.map(train_dis).fillna(0)).plot()\n",
    "        plt.legend(['train','test'])\n",
    "        plt.xlabel('&'.join(cols))\n",
    "        plt.ylabel('ratio')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df95c42-5af8-40b8-b9bb-e21ab718892a",
   "metadata": {},
   "source": [
    "能够发现所有联合变量的占比分布基本一致。数据集整体质量较高，且基本可以确认，训练集和测试集取自同一样本总体。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16064883-d4f5-4308-b08f-ca99a215624a",
   "metadata": {},
   "source": [
    "- 规律一致性分析的实际作用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d72e252-b765-4153-b49c-713963ba3d0e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在实际建模过程中，规律一致性分析是非常重要但又经常容易被忽视的一个环节。通过规律一致性分析，我们可以得出非常多的可用于后续指导后续建模的关键性意见。通常我们可以根据规律一致性分析得出以下基本结论："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e819a21-b826-486b-86a6-dcbb933cc1f8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;(1).如果分布非常一致，则说明所有特征均取自同一整体，训练集和测试集规律拥有较高一致性，模型效果上限较高，建模过程中应该更加依靠特征工程方法和模型建模技巧提高最终预测效果；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017da7a8-94a9-4708-b8d9-a1b5172cd2aa",
   "metadata": {},
   "source": [
    "&emsp;&emsp;(2).如果分布不太一致，则说明训练集和测试集规律不太一致，此时模型预测效果上限会受此影响而被限制，并且模型大概率容易过拟合，在实际建模过程中可以多考虑使用交叉验证等方式防止过拟合，并且需要注重除了通用特征工程和建模方法外的trick的使用；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63896932-f840-4382-bc33-746bca35d15e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;至此，我们就完成了核心数据集的数据探索，接下来，我们还将围绕其他的补充数据进行进一步的数据解读与数据清洗，并为最终的建模工作做好相关准备。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}